{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d75ff-0bbf-4266-aef9-f3e71ae006f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# MM pipeline: Run the StarDist2D segmentation model over all folders\n",
    "\n",
    "This notebook loads a pretrained StarDist2D segmentation model and applies the segmentation prediction on all folders within the masterfolder mainf (defined in 2nd code cell). Only microscopy chamber data containing folders should be within mainf. The segmentation is applied onto all images that end with *_PH.tif* and the segmentation image is saved into a newly created folder within each image folder named *seg_sd2*. For the moment, it assumes single-page tif files and saves single-page tif files with the exact same name as the input image used for segmentation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be194b4",
   "metadata": {},
   "source": [
    "### Load main config file. Adapt directory"
   ]
  },
  {
   "cell_type": "code",
   "id": "6037ab35-db6c-4993-95fe-2384b76475ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:46.034578900Z",
     "start_time": "2026-02-10T18:38:45.966084700Z"
    }
   },
   "source": [
    "#mainconfigname = 'jbanalysisconfig_mmrev';\n",
    "mainconfigname = 'config_example_matched';\n",
    "configdir = 'G://GitHub/microfluidics-image-processing/MM_pipeline';\n",
    "\n",
    "if not mainconfigname.endswith('.json'):\n",
    "    mainconfigname += '.json'\n",
    "    \n",
    "if not configdir.endswith('/'):\n",
    "    configdir += '/'\n",
    "\n",
    "import json\n",
    "# Read JSON data\n",
    "with open(configdir+mainconfigname, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Assign each key-value pair as a variable\n",
    "for key, value in data.items():\n",
    "    globals()[key] = value"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "8270a2bc-943c-444b-9229-11bb2c1c965f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load various packages"
   ]
  },
  {
   "cell_type": "code",
   "id": "e8362889-58a4-41ef-8062-80dbeb95f539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:56.266827300Z",
     "start_time": "2026-02-10T18:38:51.428552800Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from tifffile import imread, imwrite\n",
    "from datetime import datetime\n",
    "from csbdeep.utils import Path, normalize\n",
    "from skimage.measure import regionprops_table\n",
    "from skimage import io\n",
    "from skimage import segmentation\n",
    "from skimage import color\n",
    "from stardist.matching import matching_dataset\n",
    "from stardist import fill_label_holes, random_label_cmap, relabel_image_stardist, calculate_extents, gputools_available, _draw_polygons\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "from pathlib import Path\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()\n",
    "\n",
    "def add_prefix(props, prefix):\n",
    "    return {f\"{prefix}_{key}\" if 'intensity' in key else key: value for key, value in props.items()}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b43ae6e8-8712-4f38-9056-05fd1c397602",
   "metadata": {},
   "source": [
    "### Check if GPU can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4fbcd94-85b0-4361-8002-31aa57e2171a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:59.294870500Z",
     "start_time": "2026-02-10T18:38:58.785925200Z"
    }
   },
   "source": [
    "gputools_available()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__init__.py (276): Non-empty compiler output encountered. Set the environment variable PYOPENCL_COMPILER_OUTPUT=1 to see more.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find scikit-tensor which is needed for separable approximations...\n",
      "If you want to compute separable approximations, please install it with\n",
      "pip install scikit-tensor-py3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ea17582b-56ed-4f81-b42c-04215b86b637",
   "metadata": {},
   "source": [
    "### Load in meta file and display head. Check if correct"
   ]
  },
  {
   "cell_type": "code",
   "id": "5843fccb-8d7a-40ea-919c-2c4e49fc6d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:39:01.923712800Z",
     "start_time": "2026-02-10T18:39:01.855059800Z"
    }
   },
   "source": [
    "meta = pd.read_csv(os.path.join(masterdir,metacsv), dtype={'stardist': str})\n",
    "replicates = meta.replicate.unique()\n",
    "meta.tail()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   date replicate chip  pos  channel  Process replicate2  Process2  \\\n",
       "15  NaT       r10   c1    5        2     3818       r10b    3931.0   \n",
       "16  NaT       r11   c1    5        1     4014       r11b    4046.0   \n",
       "17  NaT       r11   c1   17        3     4026       r11b    4058.0   \n",
       "18  NaT       r11   c1   26        4     4036       r11b    4068.0   \n",
       "19  NaT       r13   c1   16        6     2567        NaN       NaN   \n",
       "\n",
       "    rep2startdifferencemin  rep2firstframe  ...        StageY  PxinUmX  \\\n",
       "15                   228.0            37.0  ... -49316.403313    0.065   \n",
       "16                    50.0             8.0  ... -43571.361981    0.065   \n",
       "17                    50.0             8.0  ... -39732.806793    0.065   \n",
       "18                    50.0             8.0  ... -37792.564308    0.065   \n",
       "19                     NaN             NaN  ... -37932.460000    0.065   \n",
       "\n",
       "   PxinUmY  register  stardist  stardist_data  stardist_data_cor  \\\n",
       "15   0.065      Done       NaN            NaN                NaN   \n",
       "16   0.065      Done       NaN            NaN                NaN   \n",
       "17   0.065      Done       NaN            NaN                NaN   \n",
       "18   0.065      Done       NaN            NaN                NaN   \n",
       "19   0.065      Done       NaN            NaN                NaN   \n",
       "\n",
       "    stardist_fails  delta delta_fails  \n",
       "15             NaN    NaN         NaN  \n",
       "16             NaN    NaN         NaN  \n",
       "17             NaN    NaN         NaN  \n",
       "18             NaN    NaN         NaN  \n",
       "19             NaN    NaN         NaN  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>replicate</th>\n",
       "      <th>chip</th>\n",
       "      <th>pos</th>\n",
       "      <th>channel</th>\n",
       "      <th>Process</th>\n",
       "      <th>replicate2</th>\n",
       "      <th>Process2</th>\n",
       "      <th>rep2startdifferencemin</th>\n",
       "      <th>rep2firstframe</th>\n",
       "      <th>...</th>\n",
       "      <th>StageY</th>\n",
       "      <th>PxinUmX</th>\n",
       "      <th>PxinUmY</th>\n",
       "      <th>register</th>\n",
       "      <th>stardist</th>\n",
       "      <th>stardist_data</th>\n",
       "      <th>stardist_data_cor</th>\n",
       "      <th>stardist_fails</th>\n",
       "      <th>delta</th>\n",
       "      <th>delta_fails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaT</td>\n",
       "      <td>r10</td>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3818</td>\n",
       "      <td>r10b</td>\n",
       "      <td>3931.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-49316.403313</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaT</td>\n",
       "      <td>r11</td>\n",
       "      <td>c1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4014</td>\n",
       "      <td>r11b</td>\n",
       "      <td>4046.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-43571.361981</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaT</td>\n",
       "      <td>r11</td>\n",
       "      <td>c1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>4026</td>\n",
       "      <td>r11b</td>\n",
       "      <td>4058.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-39732.806793</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaT</td>\n",
       "      <td>r11</td>\n",
       "      <td>c1</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>4036</td>\n",
       "      <td>r11b</td>\n",
       "      <td>4068.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-37792.564308</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaT</td>\n",
       "      <td>r13</td>\n",
       "      <td>c1</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-37932.460000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "383a362e",
   "metadata": {},
   "source": [
    "### Load stardist model\n",
    "Here, the model is loaded. You need to specify the dir which contains a folder named *stardist* in the config file. This *stardist* folder needs to contain the files *weigths_best.h5* as well as the *config.json* and optionally the *thresholds.json*"
   ]
  },
  {
   "cell_type": "code",
   "id": "da3713b5-93e4-4d12-8e15-ace7422c52e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:41:08.664371200Z",
     "start_time": "2026-02-10T18:39:04.580356700Z"
    }
   },
   "source": [
    "print(stardistmodeldir)\n",
    "model = StarDist2D(None, name='stardist', basedir=stardistmodeldir)\n",
    "axis_norm = (0,1)   # normalize channels independently"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G://GitHub/microfluidics-image-processing/stardist_models/mm\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.586968, nms_thresh=0.3.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "198a983f-591d-4329-9de0-f7351b1d17e3",
   "metadata": {},
   "source": [
    "### Define regionprops parameters. You could add more if you want to"
   ]
  },
  {
   "cell_type": "code",
   "id": "93a8cea5-1872-4fcf-98d2-804f99a854fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:09.838175900Z",
     "start_time": "2026-02-10T18:47:09.793126100Z"
    }
   },
   "source": [
    "if n_channel>1:\n",
    "    flims = True;\n",
    "    prop_list = ['label', \n",
    "                'area', 'centroid', \n",
    "                'axis_major_length', 'axis_minor_length',\n",
    "                 'eccentricity',\n",
    "                'intensity_mean', 'intensity_max']\n",
    "else:\n",
    "    flims = False;\n",
    "    prop_list = ['label', \n",
    "                'area', 'centroid', \n",
    "                'axis_major_length', 'axis_minor_length',\n",
    "                 'eccentricity'] "
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1f31f0d9-c131-4c66-9b49-57d381335eb7",
   "metadata": {},
   "source": [
    "### Limit GPU RAM usage by StarDist"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbec354f-3d21-46cd-ad68-12021a62494e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:14.422765200Z",
     "start_time": "2026-02-10T18:47:14.369218900Z"
    }
   },
   "source": [
    "from csbdeep.utils.tf import limit_gpu_memory\n",
    "# adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations\n",
    "limit_gpu_memory(fraction=ramlimit, total_memory=ramsize)\n",
    "# alternatively, try this:\n",
    "# limit_gpu_memory(None, allow_growth=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "871c295a-9117-459c-a490-959495b3665f",
   "metadata": {},
   "source": [
    "## Main segmentation loop\n",
    "This loop goes over each row in the meta file which is marked with completed preprocessing (Progress == 'Done') and applies the StarDist segmentation model to each position/chamber iteratively. For the moment, not paralellized but could probably benefit from that."
   ]
  },
  {
   "cell_type": "code",
   "id": "6820e7d4-cfce-424b-9f53-aaa823a050ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:17.895160300Z",
     "start_time": "2026-02-10T18:47:17.872149900Z"
    }
   },
   "source": [
    "# Patch Keras model's predict to always use verbose=0\n",
    "orig_predict = model.keras_model.predict\n",
    "def predict_no_verbose(*args, **kwargs):\n",
    "    kwargs['verbose'] = 0\n",
    "    return orig_predict(*args, **kwargs)\n",
    "model.keras_model.predict = predict_no_verbose"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "b6cfbbe4-bdc9-442a-a1eb-226439f5fcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:19.580915700Z",
     "start_time": "2026-02-10T18:47:19.549771200Z"
    }
   },
   "source": [
    "# helper to robustly extract trailing integer from a path basename\n",
    "def extract_trailing_int_from_basename(path):\n",
    "    name = os.path.basename(path)\n",
    "    m = re.search(r'(\\d+)$', name)\n",
    "    return int(m.group(1)) if m else None"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "f852a07c-a070-4f8e-b21e-df10d0fc29e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:13:45.725516400Z",
     "start_time": "2026-02-10T18:47:21.827825400Z"
    }
   },
   "source": [
    "# Fixed version of the processing loop to correctly extract pos and chamber numbers\n",
    "# - Removed the incorrect \"+1\" adjustment that caused Pos20 -> Pos21 in filenames/columns\n",
    "# - Robustly extracts trailing integers from folder basenames (handles Pos5, Pos05, Pos20, etc.)\n",
    "# - Uses the chamber folder as the 'folder' column so the source is exact\n",
    "# - Keeps the rest of your logic intact (stardist prediction, saving per-chamber CSVs)\n",
    "#\n",
    "# Replace your original loop with this code (or drop it into your script),\n",
    "# it only changes how pos/chamber are derived and what 'folder' is saved.\n",
    "\n",
    "# --- begin processing (replace your loop body) ---\n",
    "for i in range(0, meta.shape[0]):\n",
    "    # reload metadata each iteration if you need the file updated by others,\n",
    "    # otherwise you can read once before the loop for speed.\n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist': str, 'stardist_data': str})\n",
    "\n",
    "    if meta.loc[i, 'stardist'] == 'Done' or meta.loc[i, 'Exclude'] == 'excl' or not meta.loc[i, ('register')] == 'Done':\n",
    "        continue\n",
    "\n",
    "    main_folder = os.path.join(masterdir, savedirname, meta.replicate[i], 'Chambers')\n",
    "    save_directory = os.path.join(main_folder, 'stardist2')\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    current_directory = os.path.join(main_folder, f'Pos{str(meta.pos[i]).zfill(2)}')\n",
    "    if not os.path.exists(current_directory):\n",
    "        continue\n",
    "\n",
    "    chambf = [f.path for f in os.scandir(current_directory) if f.is_dir()]\n",
    "    chambf = [k for k in chambf if 'Chamb' in k]\n",
    "\n",
    "    fails = []\n",
    "\n",
    "    for chambi in tqdm(range(0, len(chambf)), desc=meta.replicate[i] + ', Pos ' + str(meta.pos[i]).zfill(2)):\n",
    "        inputs_folder = chambf[chambi]\n",
    "        outputs_folder = os.path.join(inputs_folder, \"seg_sd2\")\n",
    "        os.makedirs(outputs_folder, exist_ok=True)\n",
    "        for file in Path(outputs_folder).glob('*tif'):\n",
    "            os.remove(file)\n",
    "\n",
    "        images = sorted(Path(inputs_folder).glob('*Ch1*tif'))\n",
    "        if flims:\n",
    "            images_fl = sorted(Path(inputs_folder).glob('*Ch2*tif'))\n",
    "            if n_channel > 2:\n",
    "                images_fl2 = sorted(Path(inputs_folder).glob('*Ch3*tif'))\n",
    "\n",
    "        max_frame = meta.loc[i, 'MaxFr']\n",
    "        frame_list = range(len(images))\n",
    "\n",
    "        # ----> Create a DataFrame for this chamber\n",
    "        chamber_frames_df = None\n",
    "\n",
    "        # derive pos and chamber numbers robustly from folder names (no +1)\n",
    "        # If you want 1-based numbering, adjust here, but do so consciously.\n",
    "        pos_number = extract_trailing_int_from_basename(current_directory)\n",
    "        chamb_number = extract_trailing_int_from_basename(inputs_folder)\n",
    "\n",
    "        for frame_index in frame_list:\n",
    "            try:\n",
    "                if flims:\n",
    "                    fluorescence_image = imread(images_fl[frame_index])\n",
    "                    if n_channel > 2:\n",
    "                        fluorescence_image2 = imread(images_fl2[frame_index])\n",
    "\n",
    "                main_image = imread(images[frame_index])\n",
    "                normalized_image = normalize(main_image, 1, 99.8, axis=axis_norm)\n",
    "                labels, details = model.predict_instances(normalized_image, verbose=0)\n",
    "                filename_segmentation = os.path.join(outputs_folder, os.path.basename(images[frame_index]))\n",
    "                imwrite(filename_segmentation, labels, append=False, metadata=None)\n",
    "\n",
    "                region_props = regionprops_table(labels, intensity_image=fluorescence_image if flims else None, properties=prop_list)\n",
    "                if flims and n_channel > 2:\n",
    "                    region_props = add_prefix(region_props, 'fluor1')\n",
    "                    region_props2 = regionprops_table(labels, intensity_image=fluorescence_image2, properties=prop_list)\n",
    "                    region_props2 = add_prefix(region_props2, 'fluor2')\n",
    "                    for key, value in region_props2.items():\n",
    "                        if 'intensity' in key:\n",
    "                            region_props[key] = value\n",
    "\n",
    "                region_props_df = pd.DataFrame(region_props)\n",
    "\n",
    "                # Insert columns with correct values (no erroneous +1)\n",
    "                region_props_df.insert(0, 'frame', frame_index + 1)  # keep frames 1-based if desired\n",
    "                # use pos_number and chamb_number extracted from folder names\n",
    "                region_props_df.insert(0, 'pos', pos_number if pos_number is not None else meta.pos[i])\n",
    "                region_props_df.insert(0, 'replicate', meta.replicate[i])\n",
    "                # insert chamber after replicate and pos to keep a similar layout as before\n",
    "                region_props_df.insert(2, 'chamber', chamb_number if chamb_number is not None else os.path.basename(inputs_folder))\n",
    "                # use the actual chamber folder as the folder column (more precise)\n",
    "                region_props_df['folder'] = inputs_folder\n",
    "\n",
    "                if chamber_frames_df is None:\n",
    "                    chamber_frames_df = region_props_df\n",
    "                else:\n",
    "                    chamber_frames_df = pd.concat([chamber_frames_df, region_props_df], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                fails.append(f\"Error processing folder {current_directory}, Chamber {inputs_folder}, Frame {frame_index}: {e}\")\n",
    "\n",
    "        # ----> Save the DataFrame for this chamber after all frames are processed\n",
    "        if chamber_frames_df is not None:\n",
    "            # format csv filename using the extracted integers (no +1)\n",
    "            if pos_number is None:\n",
    "                pos_str = str(int(meta.pos[i])).zfill(2)\n",
    "            else:\n",
    "                pos_str = str(int(pos_number)).zfill(2)\n",
    "            if chamb_number is None:\n",
    "                # fallback: extract last two chars then zfill\n",
    "                chamb_str = os.path.basename(inputs_folder)[-2:].zfill(2)\n",
    "            else:\n",
    "                chamb_str = str(int(chamb_number)).zfill(2)\n",
    "\n",
    "            csv_filename = f\"Pos{pos_str}Chamb{chamb_str}.csv\"\n",
    "            chamber_frames_df.to_csv(os.path.join(save_directory, csv_filename), index=False)\n",
    "\n",
    "    # ----> Update metadata as before\n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist': str})\n",
    "    meta.at[i, 'stardist'] = 'Done'\n",
    "    if fails:\n",
    "        meta.at[i, 'stardist_fails'] = '; '.join(fails)\n",
    "    meta.to_csv(os.path.join(masterdir, metacsv), index=False)\n",
    "# --- end processing ---"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r07, Pos 02: 100%|██████████| 23/23 [26:23<00:00, 68.85s/it]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Done'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_37428\\3624558509.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    115\u001B[0m     \u001B[0mmeta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmasterdir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetacsv\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'stardist'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[0mmeta\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mat\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'stardist'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'Done'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m     \u001B[0mmeta\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mat\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'stardist_data'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'Done'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfails\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m         \u001B[0mmeta\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mat\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'stardist_fails'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'; '\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfails\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anconda\\envs\\jbStarDist\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   2284\u001B[0m             \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2286\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__setitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2287\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2288\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anconda\\envs\\jbStarDist\\lib\\site-packages\\pandas\\core\\indexing.py\u001B[0m in \u001B[0;36m__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   2235\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Not enough indexers for scalar access (setting)!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2236\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2237\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_set_value\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtakeable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_takeable\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2238\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2239\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anconda\\envs\\jbStarDist\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m_set_value\u001B[1;34m(self, index, col, value, takeable)\u001B[0m\n\u001B[0;32m   3824\u001B[0m             \u001B[0mvalidate_numeric_casting\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseries\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3825\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3826\u001B[1;33m             \u001B[0mseries\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3827\u001B[0m             \u001B[1;31m# Note: trying to use series._set_value breaks tests in\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3828\u001B[0m             \u001B[1;31m#  tests.frame.indexing.test_indexing and tests.indexing.test_partial\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Done'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ecd0e548-64fc-44ef-9846-d168f4c9853f",
   "metadata": {},
   "source": [
    "#DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6f916-6548-43f7-aa16-f1684d2070cc",
   "metadata": {},
   "source": [
    "below is a section to ad hoc run the data extraction from the segmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4e417-dcc7-4ae5-b7a1-c3a9e5e79eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist_data': str})\n",
    "\n",
    "for i in range(meta.shape[0]):\n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist_data': str})\n",
    "    if meta.loc[i, 'stardist_data'] == 'Done' or meta.loc[i, 'Exclude'] == 'excl' or not meta.loc[i, 'register'] == 'Done':\n",
    "        continue\n",
    "\n",
    "    # Directory setup for current experiment\n",
    "    main_folder = os.path.join(masterdir, savedirname, meta.replicate[i], 'Chambers')\n",
    "    save_directory = os.path.join(main_folder, 'stardist2')\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "    current_directory = os.path.join(main_folder, f'Pos{str(meta.pos[i]).zfill(2)}')\n",
    "    if not os.path.exists(current_directory):\n",
    "        continue\n",
    "\n",
    "    chambf = [f.path for f in os.scandir(current_directory) if f.is_dir()]\n",
    "    chambf = [k for k in chambf if 'Chamb' in k]\n",
    "\n",
    "    fails = []\n",
    "\n",
    "    for chambi in tqdm(range(0, len(chambf)), desc=meta.replicate[i] + ', Pos ' + str(meta.pos[i]).zfill(2)):\n",
    "        inputs_folder = chambf[chambi]\n",
    "        seg_outputs_folder = os.path.join(inputs_folder, \"seg_sd2\")\n",
    "        if not os.path.exists(seg_outputs_folder):\n",
    "            fails.append(f\"Missing segmentation folder: {seg_outputs_folder}\")\n",
    "            continue\n",
    "\n",
    "        seg_images = sorted(Path(seg_outputs_folder).glob('*.tif'))\n",
    "        if len(seg_images) == 0:\n",
    "            fails.append(f\"No segmentation images in {seg_outputs_folder}\")\n",
    "            continue\n",
    "\n",
    "        # Optional: fluorescence images (if needed; adjust as per your setup)\n",
    "        if flims:\n",
    "            images_fl = sorted(Path(inputs_folder).glob('*Ch2*tif'))\n",
    "            if n_channel > 2:\n",
    "                images_fl2 = sorted(Path(inputs_folder).glob('*Ch3*tif'))\n",
    "\n",
    "        # ----> Chamber-level DataFrame\n",
    "        chamber_frames_df = None\n",
    "\n",
    "        for frame_index, seg_path in enumerate(seg_images):\n",
    "            try:\n",
    "                labels = imread(seg_path)\n",
    "                if flims:\n",
    "                    fluorescence_image = imread(images_fl[frame_index])\n",
    "                    if n_channel > 2:\n",
    "                        fluorescence_image2 = imread(images_fl2[frame_index])\n",
    "\n",
    "                region_props = regionprops_table(\n",
    "                    labels,\n",
    "                    intensity_image=fluorescence_image if flims else None,\n",
    "                    properties=prop_list\n",
    "                )\n",
    "                # If >2 channels, handle as before (merge dictionaries, etc)\n",
    "\n",
    "                region_props_df = pd.DataFrame(region_props)\n",
    "                # 2-digit, 1-based indices for pos and chamber\n",
    "                pos_str = str(int(os.path.basename(current_directory)[-2:]) + 1).zfill(2)\n",
    "                chamb_str = str(int(os.path.basename(inputs_folder)[-2:]) + 1).zfill(2)\n",
    "\n",
    "                region_props_df.insert(0, 'frame', frame_index + 1)\n",
    "                region_props_df.insert(0, 'pos', int(pos_str))\n",
    "                region_props_df.insert(0, 'replicate', meta.replicate[i])\n",
    "                region_props_df.insert(2, 'chamber', int(chamb_str))\n",
    "                region_props_df['folder'] = current_directory\n",
    "\n",
    "                if chamber_frames_df is None:\n",
    "                    chamber_frames_df = region_props_df\n",
    "                else:\n",
    "                    chamber_frames_df = pd.concat([chamber_frames_df, region_props_df], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                fails.append(f\"Error processing {seg_path}: {e}\")\n",
    "\n",
    "        # ----> Save DataFrame for this chamber after all frames are processed\n",
    "        if chamber_frames_df is not None:\n",
    "            csv_filename = f\"Pos{pos_str}Chamb{chamb_str}.csv\"\n",
    "            chamber_frames_df.to_csv(os.path.join(save_directory, csv_filename), index=False)\n",
    "\n",
    "    # Update metadata\n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist_data': str})\n",
    "    meta.at[i, 'stardist_data'] = 'Done'\n",
    "    if fails:\n",
    "        meta.at[i, 'stardist_fails'] = '; '.join(fails)\n",
    "    meta.to_csv(os.path.join(masterdir, metacsv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac33d0e8-9001-4103-8b6e-2ad848eb1933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8ec376c-734f-49fc-a3f3-20565ccd7cf0",
   "metadata": {},
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0e63d8-656b-430f-8081-d2a5b602b0de",
   "metadata": {},
   "source": [
    "# CORRECT MY WRONG LABELLING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6288336-0725-4738-8ab9-a6f90c91a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging_notebook_snippet.py\n",
    "# Helper to configure the \"fix_stardist\" logger for Jupyter Lab (stdout + file)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import datetime\n",
    "\n",
    "def iso_ts():\n",
    "    return datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def configure_fix_stardist_logger(masterdir,\n",
    "                                  level=logging.INFO,\n",
    "                                  to_file=True,\n",
    "                                  maxBytes=5*1024*1024,\n",
    "                                  backupCount=5,\n",
    "                                  logger_name=\"fix_stardist\"):\n",
    "    \"\"\"\n",
    "    Configure the logger used in the fixer so INFO+ messages are visible in Jupyter cell output\n",
    "    and optionally saved to a timestamped log file under masterdir/stardist_fix_logs/.\n",
    "    This removes any existing handlers from the named logger (so previous Jupyter or library\n",
    "    handlers don't interfere), sets propagate=False, and returns the configured logger.\n",
    "\n",
    "    Parameters:\n",
    "      - masterdir: base directory where log folder will be created (used only if to_file=True)\n",
    "      - level: logging level (logging.INFO recommended)\n",
    "      - to_file: whether to create a rotating file handler in masterdir/stardist_fix_logs\n",
    "      - maxBytes, backupCount: rotating file handler params\n",
    "      - logger_name: name of the logger to configure (default 'fix_stardist')\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(logger_name)\n",
    "\n",
    "    # Remove existing handlers so previous configuration doesn't interfere\n",
    "    for h in list(log.handlers):\n",
    "        log.removeHandler(h)\n",
    "    # Prevent message propagation to root logger which might filter INFO\n",
    "    log.propagate = False\n",
    "\n",
    "    # Set level\n",
    "    log.setLevel(level)\n",
    "\n",
    "    # Stream handler to stdout so messages appear in notebook output\n",
    "    sh = logging.StreamHandler(stream=sys.stdout)\n",
    "    sh.setLevel(level)\n",
    "    sh_formatter = logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "    sh.setFormatter(sh_formatter)\n",
    "    log.addHandler(sh)\n",
    "\n",
    "    if to_file:\n",
    "        # Ensure directory exists\n",
    "        log_dir = os.path.join(masterdir, \"stardist_fix_logs\")\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_filename = os.path.join(log_dir, f\"fix_stardist_{iso_ts()}.log\")\n",
    "        fh = RotatingFileHandler(log_filename, maxBytes=maxBytes, backupCount=backupCount, encoding=\"utf-8\")\n",
    "        fh.setLevel(level)\n",
    "        fh.setFormatter(sh_formatter)\n",
    "        log.addHandler(fh)\n",
    "        log.info(f\"Logging started. File: {log_filename}\")\n",
    "\n",
    "    # show handler info for quick confirmation in notebook\n",
    "    def _show_handlers():\n",
    "        print(f\"Logger '{logger_name}' configured. Level: {logging.getLevelName(log.level)}\")\n",
    "        for i, h in enumerate(log.handlers):\n",
    "            fname = getattr(h, \"baseFilename\", None)\n",
    "            print(f\" Handler {i}: {h.__class__.__name__}, level={logging.getLevelName(h.level)}, file={fname}\")\n",
    "    _show_handlers()\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cfde4-01c5-48bb-8fea-fabf87aea6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logging to both console and a timestamped file\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import os\n",
    "\n",
    "# place this after you define iso_ts() so iso_ts() can be used for filename\n",
    "log_dir = os.path.join(masterdir, \"stardist_fix_logs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_filename = os.path.join(log_dir, f\"fix_stardist_{iso_ts()}.log\")\n",
    "\n",
    "# create logger\n",
    "log = logging.getLogger(\"fix_stardist\")\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# remove any existing handlers (useful if reloading in notebook)\n",
    "for h in list(log.handlers):\n",
    "    log.removeHandler(h)\n",
    "\n",
    "# console handler (stream)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "\n",
    "# file handler (rotating, keep up to 5 files of 5MB each)\n",
    "fh = RotatingFileHandler(log_filename, maxBytes=5*1024*1024, backupCount=5, encoding=\"utf-8\")\n",
    "fh.setLevel(logging.INFO)\n",
    "\n",
    "# formatter\n",
    "fmt = logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "ch.setFormatter(fmt)\n",
    "fh.setFormatter(fmt)\n",
    "\n",
    "# attach handlers\n",
    "log.addHandler(ch)\n",
    "log.addHandler(fh)\n",
    "\n",
    "log.info(f\"Logging started. Log file: {log_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b03b59-69b6-4c2d-afb6-fb3451dcdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = configure_fix_stardist_logger(masterdir=r\"E:\\Julian\\agr_rev_matched\", level=logging.INFO, to_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64038ea9-294f-4d03-8885-842aac1ceedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Scan all replicates in meta and fix Pos/Chamber labeling mistakes in CSVs under each replicate's\n",
    "Chambers/stardist2 folder.\n",
    "\n",
    "This version updates warning messages to include the current replicate ID so warnings like:\n",
    "  \"CSV Pos38Chamb36.csv is empty (no rows). Skipping correction for safety.\"\n",
    "become:\n",
    "  \"replicate r05 WARNING: CSV Pos38Chamb36.csv is empty (no rows). Skipping correction for safety.\"\n",
    "\n",
    "Behavior (summary):\n",
    " - iterate per replicate found in the meta CSV\n",
    " - inspect all CSVs in <masterdir>/<savedirname>/<replicate>/Chambers/stardist2\n",
    " - derive authoritative Pos from the 'folder' column if present; otherwise fall back\n",
    " - fix chamber by removing the previous +1 error when folder lacks chamber info\n",
    " - ensure 'folder' column contains ...\\PosXX\\ChambYY\n",
    " - write corrected CSVs into stardist2/corrected then move them back with conflict handling\n",
    " - update meta.stardist_data_cor='Done' for rows with that replicate where stardist_data == 'Done'\n",
    " - improved logging: warnings include replicate id\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import hashlib\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ------------- Configuration (edit to match your environment) -------------\n",
    "#masterdir = r\"E:\\Julian\\agr_rev_matched\"   # path to master directory\n",
    "#metacsv = \"matched_meta_processing.csv\"                       # metadata filename (inside masterdir)\n",
    "#savedirname = \"savesV1\"                    # under masterdir/<savedirname>/<replicate>/Chambers\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Metadata column names (adjust if your meta uses different names)\n",
    "COL_REPLICATE = \"replicate\"\n",
    "COL_POS = \"pos\"\n",
    "COL_STARDIST_DATA = \"stardist_data\"\n",
    "COL_STARDIST_DATA_COR = \"stardist_data_cor\"\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "log = logging.getLogger(\"fix_stardist\")\n",
    "\n",
    "# Helpers\n",
    "def iso_ts():\n",
    "    return datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def hash_file(path, chunk_size=65536):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def extract_pos_from_path(folder_path):\n",
    "    if not folder_path or not isinstance(folder_path, str):\n",
    "        return None\n",
    "    # look for 'Pos' followed by digits\n",
    "    m = re.search(r'Pos0*([0-9]+)', folder_path, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback: any trailing number in basename\n",
    "    base = os.path.basename(folder_path)\n",
    "    m2 = re.search(r'(\\d+)$', base)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_chamb_from_path(folder_path):\n",
    "    if not folder_path or not isinstance(folder_path, str):\n",
    "        return None\n",
    "    m = re.search(r'Chamb0*([0-9]+)', folder_path, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_pos_from_filename(fname):\n",
    "    m = re.search(r'Pos0*([0-9]+)', fname, flags=re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def extract_chamb_from_filename(fname):\n",
    "    m = re.search(r'Chamb0*([0-9]+)', fname, flags=re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def safe_makedirs(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def normalize_folder_sep(p):\n",
    "    if not isinstance(p, str):\n",
    "        return p\n",
    "    return p.replace(\"/\", os.sep).replace(\"\\\\\", os.sep)\n",
    "\n",
    "# The main function\n",
    "def fix_by_replicate(masterdir, metacsv, savedirname, dry_run=False):\n",
    "    meta_path = os.path.join(masterdir, metacsv)\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise FileNotFoundError(f\"Metadata file not found: {meta_path}\")\n",
    "    meta = pd.read_csv(meta_path, dtype=str).fillna(\"\")\n",
    "    if COL_REPLICATE not in meta.columns:\n",
    "        raise KeyError(f\"Expected metadata column '{COL_REPLICATE}' not found.\")\n",
    "\n",
    "    replicates = sorted(meta[COL_REPLICATE].unique())\n",
    "    log.info(f\"Found {len(replicates)} unique replicate(s) in metadata.\")\n",
    "\n",
    "    overall_corrections = []\n",
    "    processed_reps = set()\n",
    "\n",
    "    for rep in replicates:\n",
    "        if not rep:\n",
    "            continue\n",
    "        log.info(f\"Processing replicate: {rep}\")\n",
    "        stardist2 = os.path.join(masterdir, savedirname, rep, \"Chambers\", \"stardist2\")\n",
    "        if not os.path.isdir(stardist2):\n",
    "            log.info(f\"  No stardist2 folder for replicate {rep} at {stardist2}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        corrected_dir = os.path.join(stardist2, \"corrected\")\n",
    "        safe_makedirs(corrected_dir)\n",
    "\n",
    "        csv_files = list(Path(stardist2).glob(\"*.csv\"))\n",
    "        # Exclude anything already in corrected (just in case)\n",
    "        csv_files = [p for p in csv_files if str(p.parent) != str(Path(corrected_dir))]\n",
    "        log.info(f\"  Found {len(csv_files)} CSV(s) to inspect in {stardist2}.\")\n",
    "\n",
    "        rep_corrections = 0\n",
    "        for csv_path in csv_files:\n",
    "            csv_path = Path(csv_path)\n",
    "            fname = csv_path.name\n",
    "            log.info(f\"    Inspecting file: {fname}\")\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "            except Exception as e:\n",
    "                log.error(f\"{rep} ERROR: reading {fname}: {e}\")\n",
    "                overall_corrections.append((rep, fname, None, f\"read_error:{e}\"))\n",
    "                continue\n",
    "\n",
    "            # If dataframe is empty, skip processing to avoid iloc[0] errors.\n",
    "            if df.empty:\n",
    "                log.warning(f\"{rep} WARNING: CSV {fname} is empty (no rows). Skipping correction for safety.\")\n",
    "                overall_corrections.append((rep, fname, None, \"empty_csv_skipped\"))\n",
    "                continue\n",
    "\n",
    "            # find folder column authoritative value (first non-empty)\n",
    "            folder_val = None\n",
    "            if \"folder\" in df.columns:\n",
    "                non_empty = df['folder'].dropna().astype(str)\n",
    "                non_empty = non_empty[non_empty.str.strip() != \"\"]\n",
    "                if len(non_empty) > 0:\n",
    "                    folder_val = non_empty.iloc[0]\n",
    "            if folder_val:\n",
    "                folder_val = normalize_folder_sep(folder_val)\n",
    "\n",
    "            # extract positions/chambers\n",
    "            pos_from_folder = extract_pos_from_path(folder_val)\n",
    "            chamb_from_folder = extract_chamb_from_path(folder_val)\n",
    "\n",
    "            pos_from_fname = extract_pos_from_filename(fname)\n",
    "            chamb_from_fname = extract_chamb_from_filename(fname)\n",
    "\n",
    "            # also look into columns if present (safe because df is not empty)\n",
    "            pos_from_col = None\n",
    "            chamb_from_col = None\n",
    "            if 'pos' in df.columns:\n",
    "                try:\n",
    "                    pos_from_col = int(df['pos'].iloc[0])\n",
    "                except Exception:\n",
    "                    pos_from_col = None\n",
    "            if 'chamber' in df.columns:\n",
    "                try:\n",
    "                    chamb_from_col = int(df['chamber'].iloc[0])\n",
    "                except Exception:\n",
    "                    chamb_from_col = None\n",
    "\n",
    "            log.info(\n",
    "                f\"      extracted -> folder pos: {pos_from_folder}, folder chamb: {chamb_from_folder}, \"\n",
    "                f\"fname pos: {pos_from_fname}, fname chamb: {chamb_from_fname}, \"\n",
    "                f\"column pos: {pos_from_col}, column chamb: {chamb_from_col}\"\n",
    "            )\n",
    "\n",
    "            # Decide corrected pos\n",
    "            corrected_pos = None\n",
    "            if pos_from_folder is not None:\n",
    "                corrected_pos = pos_from_folder\n",
    "            elif pos_from_col is not None:\n",
    "                corrected_pos = pos_from_col\n",
    "            elif pos_from_fname is not None:\n",
    "                corrected_pos = pos_from_fname\n",
    "\n",
    "            if corrected_pos is None:\n",
    "                log.warning(f\"{rep} WARNING: Could not determine Pos for {fname}; skipping.\")\n",
    "                overall_corrections.append((rep, fname, None, \"no_pos_determined\"))\n",
    "                continue\n",
    "\n",
    "            # Decide corrected chamber\n",
    "            corrected_chamb = None\n",
    "            # Prefer folder chamb if present\n",
    "            if chamb_from_folder is not None:\n",
    "                corrected_chamb = chamb_from_folder\n",
    "            else:\n",
    "                # If folder lacks chamber but filename or column has it, handle the common +1 error.\n",
    "                source_ch = None\n",
    "                src = None\n",
    "                if chamb_from_col is not None:\n",
    "                    source_ch = chamb_from_col\n",
    "                    src = \"col\"\n",
    "                elif chamb_from_fname is not None:\n",
    "                    source_ch = chamb_from_fname\n",
    "                    src = \"fname\"\n",
    "\n",
    "                if source_ch is None:\n",
    "                    # no chamber information anywhere; skip\n",
    "                    log.warning(f\"{rep} WARNING: No chamber info found for {fname}; skipping.\")\n",
    "                    overall_corrections.append((rep, fname, None, \"no_chamber_info\"))\n",
    "                    continue\n",
    "\n",
    "                # If source_ch likely has +1 error, decrement by 1; otherwise keep it.\n",
    "                # Heuristic: user reported systemic +1 error -> decrement source by 1 (conservative: only if > 0)\n",
    "                if source_ch > 0:\n",
    "                    log.info(f\"      Using chamber from {src}: {source_ch}. Applying -1 fix because folder lacks chamber info.\")\n",
    "                    corrected_chamb = source_ch - 1\n",
    "                    if corrected_chamb < 0:\n",
    "                        corrected_chamb = 0\n",
    "                else:\n",
    "                    corrected_chamb = source_ch\n",
    "\n",
    "            if corrected_chamb is None:\n",
    "                log.warning(f\"{rep} WARNING: Could not determine corrected chamber for {fname}; skipping.\")\n",
    "                overall_corrections.append((rep, fname, None, \"no_corrected_chamber\"))\n",
    "                continue\n",
    "\n",
    "            # Ensure corrected_chamb and corrected_pos are ints\n",
    "            try:\n",
    "                corrected_chamb = int(corrected_chamb)\n",
    "                corrected_pos = int(corrected_pos)\n",
    "            except Exception as e:\n",
    "                log.error(f\"{rep} ERROR: converting corrected pos/chamber to int for {fname}: {e}\")\n",
    "                overall_corrections.append((rep, fname, None, f\"int_conv_error:{e}\"))\n",
    "                continue\n",
    "\n",
    "            # Compose new filename\n",
    "            new_fname = f\"Pos{str(corrected_pos).zfill(2)}Chamb{str(corrected_chamb).zfill(2)}.csv\"\n",
    "            new_path = os.path.join(corrected_dir, new_fname)\n",
    "\n",
    "            # Update dataframe pos/chamber columns\n",
    "            df['pos'] = corrected_pos\n",
    "            df['chamber'] = corrected_chamb\n",
    "\n",
    "            # Update folder column to include \\ChambNN if not present (and we have a folder base)\n",
    "            if folder_val:\n",
    "                pos_match = re.search(r'(.*Pos0*[0-9]+)', folder_val, flags=re.IGNORECASE)\n",
    "                if pos_match:\n",
    "                    base_pos_path = pos_match.group(1)\n",
    "                else:\n",
    "                    base_pos_path = folder_val\n",
    "                desired_folder = os.path.join(base_pos_path, f\"Chamb{str(corrected_chamb).zfill(2)}\")\n",
    "                desired_folder = normalize_folder_sep(desired_folder)\n",
    "                df['folder'] = desired_folder\n",
    "                log.info(f\"      Set folder to: {desired_folder}\")\n",
    "            else:\n",
    "                plausible = os.path.join(masterdir, savedirname, rep, \"Chambers\", f\"Pos{str(corrected_pos).zfill(2)}\", f\"Chamb{str(corrected_chamb).zfill(2)}\")\n",
    "                plausible = normalize_folder_sep(plausible)\n",
    "                df['folder'] = plausible\n",
    "                log.info(f\"      No folder in CSV; set folder to plausible path: {plausible}\")\n",
    "\n",
    "            # Save corrected CSV into corrected_dir\n",
    "            try:\n",
    "                if dry_run:\n",
    "                    log.info(f\"{rep} [DRY RUN] Would save corrected file to {new_path} and would remove original {csv_path}\")\n",
    "                    overall_corrections.append((rep, fname, new_fname, \"dry_run_saved\"))\n",
    "                else:\n",
    "                    df.to_csv(new_path, index=False)\n",
    "                    # remove original\n",
    "                    try:\n",
    "                        os.remove(csv_path)\n",
    "                    except Exception as e:\n",
    "                        log.warning(f\"{rep} WARNING: Could not delete original file {csv_path}: {e}\")\n",
    "                    log.info(f\"      Saved corrected file as {new_fname} (original {fname} removed).\")\n",
    "                    overall_corrections.append((rep, fname, new_fname, \"renamed\"))\n",
    "                    rep_corrections += 1\n",
    "            except Exception as e:\n",
    "                log.error(f\"{rep} ERROR: saving corrected file {new_fname}: {e}\")\n",
    "                overall_corrections.append((rep, fname, new_fname, f\"save_error:{e}\"))\n",
    "                continue\n",
    "\n",
    "        # After processing all CSVs in replicate, move corrected files back into stardist2 resolving conflicts\n",
    "        if not dry_run:\n",
    "            corrected_files = list(Path(corrected_dir).glob(\"*.csv\"))\n",
    "            if corrected_files:\n",
    "                log.info(f\"  Resolving {len(corrected_files)} corrected file(s) back into {stardist2} (replicate {rep}).\")\n",
    "            for cfile in corrected_files:\n",
    "                target = os.path.join(stardist2, cfile.name)\n",
    "                try:\n",
    "                    if os.path.exists(target):\n",
    "                        h_corr = hash_file(cfile)\n",
    "                        h_targ = hash_file(target)\n",
    "                        if h_corr == h_targ:\n",
    "                            log.info(f\"    Target {cfile.name} already exists and is identical. Deleting corrected duplicate.\")\n",
    "                            cfile.unlink()\n",
    "                        else:\n",
    "                            bak_name = f\"{os.path.basename(target)}.bak_{iso_ts()}\"\n",
    "                            bak_path = os.path.join(stardist2, bak_name)\n",
    "                            log.info(f\"    Conflict: {cfile.name} differs from existing {target}. Backing up existing -> {bak_name} and replacing.\")\n",
    "                            shutil.move(target, bak_path)\n",
    "                            shutil.move(str(cfile), target)\n",
    "                    else:\n",
    "                        shutil.move(str(cfile), target)\n",
    "                        log.info(f\"    Moved corrected file into place: {cfile.name}\")\n",
    "                except Exception as e:\n",
    "                    log.error(f\"{rep} ERROR: moving corrected file {cfile.name} into place: {e}\")\n",
    "                    overall_corrections.append((rep, cfile.name, os.path.basename(target), f\"move_error:{e}\"))\n",
    "                    continue\n",
    "\n",
    "            # remove corrected_dir if empty\n",
    "            try:\n",
    "                if os.path.isdir(corrected_dir) and not any(Path(corrected_dir).iterdir()):\n",
    "                    os.rmdir(corrected_dir)\n",
    "                    log.info(f\"  Removed empty corrected directory: {corrected_dir}\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"{rep} WARNING: Could not remove corrected directory {corrected_dir}: {e}\")\n",
    "\n",
    "        processed_reps.add(rep)\n",
    "\n",
    "        log.info(f\"Finished processing replicate {rep}. Corrections applied: {rep_corrections}\")\n",
    "\n",
    "        # Update metadata: mark rows for this replicate as corrected where stardist_data == 'Done'\n",
    "        if not dry_run:\n",
    "            mask = (meta[COL_REPLICATE] == rep) & (meta.get(COL_STARDIST_DATA, \"\") == \"Done\")\n",
    "            if mask.any():\n",
    "                meta.loc[mask, COL_STARDIST_DATA_COR] = \"Done\"\n",
    "                log.info(f\"  Updated metadata stardist_data_cor='Done' for {mask.sum()} row(s) in replicate {rep}.\")\n",
    "\n",
    "    # Save updated metadata\n",
    "    if not dry_run:\n",
    "        meta.to_csv(meta_path, index=False)\n",
    "        log.info(f\"Saved updated metadata to {meta_path}\")\n",
    "\n",
    "    # Final summary\n",
    "    log.info(\"Processing complete.\")\n",
    "    log.info(f\"Total replicates processed: {len(processed_reps)}\")\n",
    "    log.info(f\"Total actions recorded: {len(overall_corrections)}\")\n",
    "    for rec in overall_corrections:\n",
    "        log.info(f\"  RECORD: {rec}\")\n",
    "\n",
    "    return overall_corrections\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example run: set dry_run=True to preview actions without changing files\n",
    "    corrections = fix_by_replicate(masterdir=masterdir, metacsv=metacsv, savedirname=savedirname, dry_run=False)\n",
    "    print(f\"Done. Total records: {len(corrections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c821c-45ae-4e57-aa8e-d520c56bf00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = fix_by_replicate(masterdir=masterdir, metacsv=metacsv, savedirname=savedirname, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6567b-61c4-45d8-aeb4-f75c74200ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = fix_by_replicate(masterdir=masterdir, metacsv=metacsv, savedirname=savedirname, dry_run=False)\n",
    "print(f\"Done. Total records: {len(corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbecbdd-7639-4e20-92a7-6623f20cdf4c",
   "metadata": {},
   "source": [
    "DONE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b57c1-7cc4-4b25-97a5-f4a52cbc13da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
