{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55d75ff-0bbf-4266-aef9-f3e71ae006f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# MM pipeline: Run the StarDist2D segmentation model over all folders\n",
    "\n",
    "This notebook loads a pretrained StarDist2D segmentation model and applies the segmentation prediction on all folders within the masterfolder mainf (defined in 2nd code cell). Only microscopy chamber data containing folders should be within mainf. The segmentation is applied onto all images that end with *_PH.tif* and the segmentation image is saved into a newly created folder within each image folder named *seg_sd2*. For the moment, it assumes single-page tif files and saves single-page tif files with the exact same name as the input image used for segmentation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be194b4",
   "metadata": {},
   "source": [
    "### Load main config file. Adapt directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6037ab35-db6c-4993-95fe-2384b76475ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:46.034578900Z",
     "start_time": "2026-02-10T18:38:45.966084700Z"
    }
   },
   "outputs": [],
   "source": [
    "configdir = 'G://GitHub/microfluidics-image-processing/MM_pipeline';\n",
    "\n",
    "# uncomment the one running:\n",
    "#mainconfigname = 'config_example_mixed';\n",
    "mainconfigname = 'config_example_matched';\n",
    "\n",
    "\n",
    "if not mainconfigname.endswith('.json'):\n",
    "    mainconfigname += '.json'\n",
    "    \n",
    "if not configdir.endswith('/'):\n",
    "    configdir += '/'\n",
    "\n",
    "import json\n",
    "# Read JSON data\n",
    "with open(configdir+mainconfigname, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Assign each key-value pair as a variable\n",
    "for key, value in data.items():\n",
    "    globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270a2bc-943c-444b-9229-11bb2c1c965f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load various packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8362889-58a4-41ef-8062-80dbeb95f539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:56.266827300Z",
     "start_time": "2026-02-10T18:38:51.428552800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from tifffile import imread, imwrite\n",
    "from datetime import datetime\n",
    "from csbdeep.utils import Path, normalize\n",
    "from skimage.measure import regionprops_table\n",
    "from skimage import io\n",
    "from skimage import segmentation\n",
    "from skimage import color\n",
    "from stardist.matching import matching_dataset\n",
    "from stardist import fill_label_holes, random_label_cmap, relabel_image_stardist, calculate_extents, gputools_available, _draw_polygons\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()\n",
    "\n",
    "def add_prefix(props, prefix):\n",
    "    return {f\"{prefix}_{key}\" if 'intensity' in key else key: value for key, value in props.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== CRITICAL: Add these at the very top of your script BEFORE any imports ==========\n",
    "os.environ['OMP_NUM_THREADS'] = '6'  # Adjust based on stability (4-8)\n",
    "os.environ['MKL_NUM_THREADS'] = '6'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '6'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '6'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '2'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '6'\n",
    "\n",
    "# Now import TensorFlow/Stardist\n",
    "import tensorflow as tf\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ae6e8-8712-4f38-9056-05fd1c397602",
   "metadata": {},
   "source": [
    "### Check if GPU can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fbcd94-85b0-4361-8002-31aa57e2171a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:38:59.294870500Z",
     "start_time": "2026-02-10T18:38:58.785925200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gputools_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea17582b-56ed-4f81-b42c-04215b86b637",
   "metadata": {},
   "source": [
    "### Load in meta file and display head. Check if correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5843fccb-8d7a-40ea-919c-2c4e49fc6d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:39:01.923712800Z",
     "start_time": "2026-02-10T18:39:01.855059800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I://Julian/agr_rev_matched/sharing/matched-try\\shared_matched_meta_processing.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>replicate</th>\n",
       "      <th>chip</th>\n",
       "      <th>pos</th>\n",
       "      <th>channel</th>\n",
       "      <th>Process</th>\n",
       "      <th>replicate2</th>\n",
       "      <th>Process2</th>\n",
       "      <th>rep2startdifferencemin</th>\n",
       "      <th>rep2firstframe</th>\n",
       "      <th>...</th>\n",
       "      <th>StageY</th>\n",
       "      <th>PxinUmX</th>\n",
       "      <th>PxinUmY</th>\n",
       "      <th>register</th>\n",
       "      <th>stardist</th>\n",
       "      <th>stardist_data</th>\n",
       "      <th>stardist_data_cor</th>\n",
       "      <th>stardist_fails</th>\n",
       "      <th>delta</th>\n",
       "      <th>delta_fails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.08.2024</td>\n",
       "      <td>r04</td>\n",
       "      <td>c1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1784</td>\n",
       "      <td>r04b</td>\n",
       "      <td>1847</td>\n",
       "      <td>145</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>-38615.71153</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.065</td>\n",
       "      <td>Done</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date replicate chip  pos  channel  Process replicate2  Process2  \\\n",
       "0  29.08.2024       r04   c1    1        2     1784       r04b      1847   \n",
       "\n",
       "   rep2startdifferencemin  rep2firstframe  ...       StageY  PxinUmX PxinUmY  \\\n",
       "0                     145              41  ... -38615.71153    0.065   0.065   \n",
       "\n",
       "   register  stardist  stardist_data  stardist_data_cor  stardist_fails  \\\n",
       "0      Done       NaN            NaN                NaN             NaN   \n",
       "\n",
       "   delta delta_fails  \n",
       "0    NaN         NaN  \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.path.join(masterdir,metacsv))\n",
    "meta = pd.read_csv(os.path.join(masterdir,metacsv), dtype={'stardist': str})\n",
    "replicates = meta.replicate.unique()\n",
    "meta.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a362e",
   "metadata": {},
   "source": [
    "### Load stardist model\n",
    "Here, the model is loaded. You need to specify the dir which contains a folder named *stardist* in the config file. This *stardist* folder needs to contain the files *weigths_best.h5* as well as the *config.json* and optionally the *thresholds.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3713b5-93e4-4d12-8e15-ace7422c52e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:41:08.664371200Z",
     "start_time": "2026-02-10T18:39:04.580356700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G://GitHub/microfluidics-image-processing/stardist_models/mm\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.586968, nms_thresh=0.3.\n"
     ]
    }
   ],
   "source": [
    "print(stardistmodeldir)\n",
    "model = StarDist2D(None, name='stardist', basedir=stardistmodeldir)\n",
    "axis_norm = (0,1)   # normalize channels independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a983f-591d-4329-9de0-f7351b1d17e3",
   "metadata": {},
   "source": [
    "### Define regionprops parameters. You could add more if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a8cea5-1872-4fcf-98d2-804f99a854fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:09.838175900Z",
     "start_time": "2026-02-10T18:47:09.793126100Z"
    }
   },
   "outputs": [],
   "source": [
    "if n_channel>1:\n",
    "    flims = True;\n",
    "    prop_list = ['label', \n",
    "                'area', 'centroid', \n",
    "                'axis_major_length', 'axis_minor_length',\n",
    "                 'eccentricity',\n",
    "                'intensity_mean', 'intensity_max']\n",
    "else:\n",
    "    flims = False;\n",
    "    prop_list = ['label', \n",
    "                'area', 'centroid', \n",
    "                'axis_major_length', 'axis_minor_length',\n",
    "                 'eccentricity'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31f0d9-c131-4c66-9b49-57d381335eb7",
   "metadata": {},
   "source": [
    "### Limit GPU RAM usage by StarDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbec354f-3d21-46cd-ad68-12021a62494e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T18:47:14.422765200Z",
     "start_time": "2026-02-10T18:47:14.369218900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from csbdeep.utils.tf import limit_gpu_memory\n",
    "# adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations\n",
    "limit_gpu_memory(fraction=ramlimit, total_memory=ramsize)\n",
    "# alternatively, try this:\n",
    "# limit_gpu_memory(None, allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c295a-9117-459c-a490-959495b3665f",
   "metadata": {},
   "source": [
    "## Main segmentation loop\n",
    "This loop goes over each row in the meta file which is marked with completed preprocessing (Progress == 'Done') and applies the StarDist segmentation model to each position/chamber iteratively. For the moment, not paralellized but could probably benefit from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f852a07c-a070-4f8e-b21e-df10d0fc29e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T19:13:45.725516400Z",
     "start_time": "2026-02-10T18:47:21.827825400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU mode with 2 threads\n",
      "\n",
      "[INFO] Processing r04, Pos 01 with 43 chambers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=2)]: Done  43 out of  43 | elapsed: 43.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Successfully processed 43/43 chambers\n",
      "\n",
      "============================================================\n",
      "STARDIST SEGMENTATION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== Helper Functions ==========\n",
    "# Patch Keras model's predict to always use verbose=0\n",
    "orig_predict = model.keras_model.predict\n",
    "def predict_no_verbose(*args, **kwargs):\n",
    "    kwargs['verbose'] = 0\n",
    "    return orig_predict(*args, **kwargs)\n",
    "model.keras_model.predict = predict_no_verbose\n",
    "\n",
    "# helper to robustly extract trailing integer from a path basename\n",
    "def extract_trailing_int_from_basename(path):\n",
    "    name = os.path.basename(path)\n",
    "    m = re.search(r'(\\d+)$', name)\n",
    "    return int(m.group(1)) if m else None\n",
    "    \n",
    "def check_memory_and_cleanup(threshold_percent=75):\n",
    "    \"\"\"Check memory usage and force cleanup if needed\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    if mem.percent > threshold_percent:\n",
    "        print(f\"[WARNING] Memory at {mem.percent:.1f}%, forcing cleanup...\")\n",
    "        gc.collect()\n",
    "        import time\n",
    "        time.sleep(1)\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"[INFO] Memory after cleanup: {mem.percent:.1f}%\")\n",
    "    return mem.percent\n",
    "\n",
    "def process_single_chamber(chambi, chambf, inputs_folder, meta_row, model, flims, n_channel, prop_list, axis_norm):\n",
    "    \"\"\"\n",
    "    Process a single chamber independently.\n",
    "    Returns: (chamber_frames_df, fails, pos_number, chamb_number, inputs_folder)\n",
    "    \"\"\"\n",
    "    # Suppress low contrast warnings for label images\n",
    "    warnings.filterwarnings('ignore', message='.*low contrast image.*')\n",
    "    outputs_folder = os.path.join(inputs_folder, \"seg_sd2\")\n",
    "    os.makedirs(outputs_folder, exist_ok=True)\n",
    "    \n",
    "    # Clear existing files\n",
    "    for file in Path(outputs_folder).glob('*tif'):\n",
    "        os.remove(file)\n",
    "    \n",
    "    images = sorted(Path(inputs_folder).glob('*Ch1*tif'))\n",
    "    if flims:\n",
    "        images_fl = sorted(Path(inputs_folder).glob('*Ch2*tif'))\n",
    "        if n_channel > 2:\n",
    "            images_fl2 = sorted(Path(inputs_folder).glob('*Ch3*tif'))\n",
    "    \n",
    "    frame_list = range(len(images))\n",
    "    chamber_frames_df = None\n",
    "    fails = []\n",
    "    \n",
    "    # Extract position and chamber numbers\n",
    "    current_directory = os.path.dirname(inputs_folder)\n",
    "    pos_number = extract_trailing_int_from_basename(current_directory)\n",
    "    chamb_number = extract_trailing_int_from_basename(inputs_folder)\n",
    "    \n",
    "    for frame_index in frame_list:\n",
    "        try:\n",
    "            if flims:\n",
    "                fluorescence_image = imread(images_fl[frame_index])\n",
    "                if n_channel > 2:\n",
    "                    fluorescence_image2 = imread(images_fl2[frame_index])\n",
    "            \n",
    "            main_image = imread(images[frame_index])\n",
    "            normalized_image = normalize(main_image, 1, 99.8, axis=axis_norm)\n",
    "            labels, details = model.predict_instances(normalized_image, verbose=0)\n",
    "            \n",
    "            filename_segmentation = os.path.join(outputs_folder, os.path.basename(images[frame_index]))\n",
    "            imwrite(filename_segmentation, labels, append=False, metadata=None)\n",
    "            \n",
    "            region_props = regionprops_table(\n",
    "                labels, \n",
    "                intensity_image=fluorescence_image if flims else None, \n",
    "                properties=prop_list\n",
    "            )\n",
    "            \n",
    "            if flims and n_channel > 2:\n",
    "                region_props = add_prefix(region_props, 'fluor1')\n",
    "                region_props2 = regionprops_table(labels, intensity_image=fluorescence_image2, properties=prop_list)\n",
    "                region_props2 = add_prefix(region_props2, 'fluor2')\n",
    "                for key, value in region_props2.items():\n",
    "                    if 'intensity' in key:\n",
    "                        region_props[key] = value\n",
    "            \n",
    "            region_props_df = pd.DataFrame(region_props)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            region_props_df.insert(0, 'frame', frame_index + 1)\n",
    "            region_props_df.insert(0, 'pos', pos_number if pos_number is not None else meta_row['pos'])\n",
    "            region_props_df.insert(0, 'replicate', meta_row['replicate'])\n",
    "            region_props_df.insert(2, 'chamber', chamb_number if chamb_number is not None else os.path.basename(inputs_folder))\n",
    "            region_props_df['folder'] = inputs_folder\n",
    "            \n",
    "            if chamber_frames_df is None:\n",
    "                chamber_frames_df = region_props_df\n",
    "            else:\n",
    "                chamber_frames_df = pd.concat([chamber_frames_df, region_props_df], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            fails.append(f\"Error processing Chamber {inputs_folder}, Frame {frame_index}: {e}\")\n",
    "    \n",
    "    return chamber_frames_df, fails, pos_number, chamb_number, inputs_folder\n",
    "\n",
    "\n",
    "# ========== Main loop with safe parallelization ==========\n",
    "\n",
    "# Determine safe number of parallel jobs\n",
    "# Start conservative - stardist is GPU-heavy if using GPU, or CPU-heavy if CPU-only\n",
    "if model.config.use_gpu:\n",
    "    # If using GPU, limit parallelization more (GPU can't be shared well)\n",
    "    n_jobs = 2  # Very conservative for GPU\n",
    "    backend = 'threading'  # Threading shares GPU better\n",
    "    print(f\"[INFO] Using GPU mode with {n_jobs} threads\")\n",
    "else:\n",
    "    # If using CPU, can parallelize more\n",
    "    n_jobs = min(4, multiprocessing.cpu_count() - 2)  # Leave 2 cores free\n",
    "    backend = 'loky'  # Multiprocessing for CPU\n",
    "    print(f\"[INFO] Using CPU mode with {n_jobs} workers\")\n",
    "\n",
    "for i in range(0, meta.shape[0]):\n",
    "    # Check memory before starting a new position\n",
    "    mem_percent = check_memory_and_cleanup(threshold_percent=70)\n",
    "    if mem_percent > 85:\n",
    "        print(f\"[ERROR] Memory too high ({mem_percent:.1f}%), skipping position {i}\")\n",
    "        continue\n",
    "    \n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist': str, 'stardist_data': str})\n",
    "    \n",
    "    if meta.loc[i, 'stardist'] == 'Done' or meta.loc[i, 'Exclude'] == 'excl' or not meta.loc[i, ('register')] == 'Done':\n",
    "        continue\n",
    "    \n",
    "    main_folder = os.path.join(masterdir, savedirname, meta.replicate[i], 'Chambers')\n",
    "    save_directory = os.path.join(main_folder, 'stardist2')\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    current_directory = os.path.join(main_folder, f'Pos{str(meta.pos[i]).zfill(2)}')\n",
    "    if not os.path.exists(current_directory):\n",
    "        print(current_directory + ' not found')\n",
    "        continue\n",
    "    \n",
    "    chambf = [f.path for f in os.scandir(current_directory) if f.is_dir()]\n",
    "    chambf = [k for k in chambf if 'Chamb' in k]\n",
    "    \n",
    "    if len(chambf) == 0:\n",
    "        print(f\"[WARN] No chambers found in {current_directory}\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    meta_row = meta.iloc[i].to_dict()\n",
    "    \n",
    "    # Process all chambers in parallel\n",
    "    print(f\"\\n[INFO] Processing {meta.replicate[i]}, Pos {str(meta.pos[i]).zfill(2)} with {len(chambf)} chambers...\")\n",
    "    \n",
    "    try:\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend, verbose=5)(\n",
    "            delayed(process_single_chamber)(\n",
    "                chambi, chambf, chambf[chambi], meta_row, model, \n",
    "                flims, n_channel, prop_list, axis_norm\n",
    "            ) for chambi in range(len(chambf))\n",
    "        )\n",
    "        \n",
    "        # Collect results and save CSVs\n",
    "        all_fails = []\n",
    "        successful_chambers = 0\n",
    "        \n",
    "        for chamber_frames_df, fails, pos_number, chamb_number, inputs_folder in results:\n",
    "            all_fails.extend(fails)\n",
    "            \n",
    "            if chamber_frames_df is not None and len(chamber_frames_df) > 0:\n",
    "                successful_chambers += 1\n",
    "                \n",
    "                # Format csv filename\n",
    "                if pos_number is None:\n",
    "                    pos_str = str(int(meta.pos[i])).zfill(2)\n",
    "                else:\n",
    "                    pos_str = str(int(pos_number)).zfill(2)\n",
    "                if chamb_number is None:\n",
    "                    chamb_str = os.path.basename(inputs_folder)[-2:].zfill(2)\n",
    "                else:\n",
    "                    chamb_str = str(int(chamb_number)).zfill(2)\n",
    "                \n",
    "                csv_filename = f\"Pos{pos_str}Chamb{chamb_str}.csv\"\n",
    "                chamber_frames_df.to_csv(os.path.join(save_directory, csv_filename), index=False)\n",
    "        \n",
    "        print(f\"[INFO] Successfully processed {successful_chambers}/{len(chambf)} chambers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Parallel processing failed for position {i}: {e}\")\n",
    "        all_fails = [f\"Parallel processing error: {e}\"]\n",
    "    \n",
    "    # Force cleanup after each position\n",
    "    gc.collect()\n",
    "    \n",
    "    # Update metadata\n",
    "    meta = pd.read_csv(os.path.join(masterdir, metacsv), dtype={'stardist': str})\n",
    "    meta.at[i, 'stardist'] = 'Done'\n",
    "    if all_fails:\n",
    "        meta.at[i, 'stardist_fails'] = '; '.join(all_fails)\n",
    "    meta.to_csv(os.path.join(masterdir, metacsv), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARDIST SEGMENTATION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec376c-734f-49fc-a3f3-20565ccd7cf0",
   "metadata": {},
   "source": [
    "### DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc3a5a-bd31-4469-930e-938dac3a37fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
